{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "import numpy as np\n",
    "import json\n",
    "from xml.dom import minidom\n",
    "import os\n",
    "import abc\n",
    "\n",
    "\n",
    "# mean squared error\n",
    "def mse(y_true, y_pred, status=''):\n",
    "    return mse_derivative(y_true, y_pred) if status=='derivative' else np.mean(np.power(y_true-y_pred, 2))\n",
    "\n",
    "\n",
    "# sigmoid\n",
    "def sigmoid(input_data, status=''):\n",
    "    return sigmoid_derivative(input_data) if status =='derivative' else 1/(1 + np.ex(-input_data))\n",
    "\n",
    "\n",
    "# tg() activation function \n",
    "def tanh(input_data, status=''):\n",
    "    return tanh_derivative(input_data) if status=='derivative' else np.tanh(input_data);\n",
    "\n",
    "\n",
    "#  relu activation function\n",
    "def relu(input_data, status=''):\n",
    "    if status=='derivative':\n",
    "        return relu_derivative(input_data)\n",
    "    data = [max(0.05*value, value) for array in input_data for value in array]\n",
    "    return np.array(data).reshape(input_data.shape)\n",
    "\n",
    "\n",
    "# derivative of mean squared error\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size\n",
    "\n",
    "\n",
    "# derivative of sigmoid\n",
    "def sigmoid_derivative(input_data):\n",
    "    return sigmoid(input_data)*(1 - sigmoid(input_data))\n",
    "\n",
    "\n",
    "# derivative of tag() activation functions\n",
    "def tanh_derivative(input_data):\n",
    "    return 1-np.tanh(input_data)**2\n",
    "\n",
    "\n",
    "# derivative of relu\n",
    "def relu_derivative(input_data):\n",
    "    data = [1 if value > 0 else 0.05 for array in input_data for value in array]\n",
    "    return np.array(data).reshape(input_data.shape)\n",
    "\n",
    "\n",
    "# calculate gradient\n",
    "def calculate_gradients(input_data, weights, bias, loss):\n",
    "    # db==loss\n",
    "    input_loss = np.dot(loss, weights.T)\n",
    "    dw = np.dot(input_data.T, loss)\n",
    "\n",
    "    return input_loss, dw, loss\n",
    "\n",
    "\n",
    "# get the true value\n",
    "def get_value(array):\n",
    "    arr = np.where(array==np.amax(array))\n",
    "    return int(arr[0]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(abc.ABC):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._input = None\n",
    "        self._output = None\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def forward(self, input_data):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def backward(self, error, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        self._weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self._bias = np.random.rand(1, output_size) - 0.5\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        self._input = input_data\n",
    "        self._output = np.dot(self._input, self._weights) + self._bias\n",
    "        \n",
    "        return self._output\n",
    "\n",
    "    def backward(self, loss, optimizer):\n",
    "        self._weights, self._bias, input_loss = optimizer.update(self._input, self._weights, self._bias, loss)\n",
    "        \n",
    "        return input_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    \n",
    "    def __init__(self, activation_function):\n",
    "        self._activation_function = activation_function\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self._input = input_data\n",
    "        self._output = self._activation_function(self._input)\n",
    "        \n",
    "        return self._output\n",
    "\n",
    "    def backward(self, loss, optimizer):\n",
    "        return self._activation_function(self._input, 'derivative') * loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._layers = []\n",
    "        self._json = {}\n",
    "        self._root = minidom.Document()\n",
    "        \n",
    "        self._xml_results = self._root.createElement('results') \n",
    "        self._root.appendChild(self._xml_results)\n",
    "\n",
    "    def add(self, layer):\n",
    "        self._layers.append(layer)\n",
    "\n",
    "    def fit(self, x_train, y_train, epochs, optimizer, loss_func):\n",
    "        # dictionary for epoch results storing\n",
    "        d = {}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            loss_display = 0\n",
    "            for i in range(len(x_train)):\n",
    "                output = x_train[i]\n",
    "                \n",
    "                # forward propagation\n",
    "                for layer in self._layers:\n",
    "                    output = layer.forward(output)\n",
    "                \n",
    "                # calculating errors (1st to display, 2nd for backpropagataion)\n",
    "                loss_display += loss_func(y_train[i], output)\n",
    "                loss = loss_func(y_train[i], output, 'derivative')\n",
    "                \n",
    "                # backward propagation\n",
    "                for layer in reversed(self._layers):\n",
    "                    loss = layer.backward(loss, optimizer)\n",
    "            \n",
    "            loss_epoch = loss_display/len(x_train)\n",
    "            print('Epoch: {}/{}  Loss: {}'.format(epoch + 1, epochs, loss_epoch))\n",
    "            \n",
    "            d['Epoch: {}/{}'.format(epoch + 1, epochs)] = loss_epoch\n",
    "            self._xml_results.appendChild(self.xml_element('epoch: {}/{}'.format(epoch + 1, epochs), 'loss', str(loss_epoch)))\n",
    "        \n",
    "        self._json['Training'] = d\n",
    "        self._json['Optimizer'] = type(optimizer).__name__\n",
    "        self._json['Loss Function'] = loss_func.__name__\n",
    "        \n",
    "        self._xml_results.appendChild(self.xml_element('optimizer', 'name', str(type(optimizer).__name__)))\n",
    "        self._xml_results.appendChild(self.xml_element('loss function', 'name', str(loss_func.__name__)))\n",
    "            \n",
    "    def predict(self, input_data, true_value):\n",
    "        predictions = []\n",
    "        \n",
    "        # run network over all samples\n",
    "        for i in range(len(input_data)):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self._layers:\n",
    "                output = layer.forward(output)\n",
    "                \n",
    "            output = output.flatten()\n",
    "            predictions.append(get_value(output)==get_value(true_value[i]))\n",
    "            \n",
    "        accuracy = (sum(predictions)/len(predictions))*100\n",
    "        print('Accuracy: {}'.format(accuracy))\n",
    "        self._json['Accuracy'] = accuracy\n",
    "        self._xml_results.appendChild(self.xml_element('accuracy', 'value', str(accuracy)))\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def create_json(self, fname):\n",
    "        json_object = json.dumps(self._json, indent = 4)\n",
    "        \n",
    "        with open(fname, \"w\") as outfile:\n",
    "            outfile.write(json_object)\n",
    "    \n",
    "    def create_xml(self, fname):\n",
    "        xml_str = self._root.toprettyxml(indent =\"\\t\") \n",
    "        \n",
    "        with open(fname, \"w\") as f:\n",
    "            f.write(xml_str) \n",
    "    \n",
    "    def xml_element(self, name_element, name_info, info):\n",
    "        xml_element = self._root.createElement(name_element)\n",
    "        xml_element.setAttribute(name_info, info)\n",
    "        return xml_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    \n",
    "    def __init__(self, learning_rate):\n",
    "        self._learning_rate = learning_rate\n",
    "        \n",
    "    def update(self, input_data, weights, bias, loss):\n",
    "        input_loss, dw, db = calculate_gradients(input_data, weights, bias, loss)\n",
    "\n",
    "        weights -= self._learning_rate * dw\n",
    "        bias -= self._learning_rate * db\n",
    "        \n",
    "        return weights, bias, input_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# UNDER DEVELOPMENT\n",
    "# NOT READY TO WORK YET\n",
    "class Adam():\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self._m_dw, self._v_dw = 0, 0\n",
    "        self._m_db, self._v_db = 0, 0\n",
    "        self._beta1 = beta1\n",
    "        self._beta2 = beta2\n",
    "        self._epsilon = epsilon\n",
    "        self._learning_rate = learning_rate\n",
    "        \n",
    "    def update(self, input_data, weights, bias, loss):\n",
    "        input_loss, dw, db = calculate_gradients(input_data, weights, bias, loss)\n",
    "        \n",
    "        ## momentum beta 1\n",
    "        self._m_dw = self._beta1*self._m_dw + (1-self._beta1)*dw\n",
    "        self._m_db = self._beta1*self._m_db + (1-self._beta1)*db\n",
    "\n",
    "        ## rms beta 2\n",
    "        self._v_dw = self._beta2*self._v_dw + (1-self._beta2)*(dw**2)\n",
    "        self._v_db = self._beta2*self._v_db + (1-self._beta2)*(db)\n",
    "\n",
    "        ## bias correction\n",
    "        m_dw_corr = self._m_dw/(1-self._beta1)\n",
    "        m_db_corr = self._m_db/(1-self._beta1)\n",
    "        v_dw_corr = self._v_dw/(1-self._beta2)\n",
    "        v_db_corr = self._v_db/(1-self._beta2)\n",
    "\n",
    "        ## update weights and biases\n",
    "        weights = weights - self._learning_rate*(m_dw_corr/(np.sqrt(v_dw_corr) + self._epsilon))\n",
    "        bias = bias - self._learning_rate*(m_db_corr/(np.sqrt(v_db_corr) + self._epsilon))\n",
    "        \n",
    "        return weights, bias, input_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "# load data \n",
    "iris = load_iris()\n",
    "x, y = np.array(iris.data), np.array(iris.target)\n",
    "\n",
    "# split on training and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# preprocess train and test data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 4)\n",
    "x_train = x_train.astype('float32')\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 4)\n",
    "x_test = x_test.astype('float32')\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# optimizer\n",
    "sgd = SGD(learning_rate=0.1)\n",
    "optimizer_2 = Adam()\n",
    "\n",
    "# network\n",
    "net = NeuralNetwork()\n",
    "net.add(Linear(4, 20))\n",
    "net.add(Activation(relu))\n",
    "net.add(Linear(20, 10))\n",
    "net.add(Activation(relu))\n",
    "net.add(Linear(10, 3))\n",
    "net.add(Activation(relu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/200  Loss: 0.293187399227887\n",
      "Epoch: 2/200  Loss: 0.21721414697593586\n",
      "Epoch: 3/200  Loss: 0.11006963570703339\n",
      "Epoch: 4/200  Loss: 0.10630851332562698\n",
      "Epoch: 5/200  Loss: 0.09940955696815704\n",
      "Epoch: 6/200  Loss: 0.09588591161783379\n",
      "Epoch: 7/200  Loss: 0.09252207907931015\n",
      "Epoch: 8/200  Loss: 0.08262753792029451\n",
      "Epoch: 9/200  Loss: 0.08197370637425162\n",
      "Epoch: 10/200  Loss: 0.062363066186503016\n",
      "Epoch: 11/200  Loss: 0.0688949603542168\n",
      "Epoch: 12/200  Loss: 0.13145183376613104\n",
      "Epoch: 13/200  Loss: 0.09846839772397589\n",
      "Epoch: 14/200  Loss: 0.07602537964974522\n",
      "Epoch: 15/200  Loss: 0.05731526776294095\n",
      "Epoch: 16/200  Loss: 0.056543026413847806\n",
      "Epoch: 17/200  Loss: 0.05378139702807595\n",
      "Epoch: 18/200  Loss: 0.05148441770967936\n",
      "Epoch: 19/200  Loss: 0.046364421915879875\n",
      "Epoch: 20/200  Loss: 0.039725753549521535\n",
      "Epoch: 21/200  Loss: 0.04138327712065658\n",
      "Epoch: 22/200  Loss: 0.0409068980502013\n",
      "Epoch: 23/200  Loss: 0.04064608766688323\n",
      "Epoch: 24/200  Loss: 0.0401972643316255\n",
      "Epoch: 25/200  Loss: 0.036851232046610095\n",
      "Epoch: 26/200  Loss: 0.03801215168491138\n",
      "Epoch: 27/200  Loss: 0.036103655491005644\n",
      "Epoch: 28/200  Loss: 0.03704068077273615\n",
      "Epoch: 29/200  Loss: 0.03661924737238754\n",
      "Epoch: 30/200  Loss: 0.039073398225156236\n",
      "Epoch: 31/200  Loss: 0.037409856266691\n",
      "Epoch: 32/200  Loss: 0.03450012833246264\n",
      "Epoch: 33/200  Loss: 0.034474876660375436\n",
      "Epoch: 34/200  Loss: 0.0345712940540543\n",
      "Epoch: 35/200  Loss: 0.033685768792876214\n",
      "Epoch: 36/200  Loss: 0.03286548911443583\n",
      "Epoch: 37/200  Loss: 0.03741774287888926\n",
      "Epoch: 38/200  Loss: 0.037153470864548545\n",
      "Epoch: 39/200  Loss: 0.032121005816100834\n",
      "Epoch: 40/200  Loss: 0.03362060019186466\n",
      "Epoch: 41/200  Loss: 0.0343789612319134\n",
      "Epoch: 42/200  Loss: 0.03409163690421418\n",
      "Epoch: 43/200  Loss: 0.032642771338192725\n",
      "Epoch: 44/200  Loss: 0.03371575277594521\n",
      "Epoch: 45/200  Loss: 0.03325347217009458\n",
      "Epoch: 46/200  Loss: 0.03287309144503385\n",
      "Epoch: 47/200  Loss: 0.031705526791298004\n",
      "Epoch: 48/200  Loss: 0.031588133258398585\n",
      "Epoch: 49/200  Loss: 0.03183822208257802\n",
      "Epoch: 50/200  Loss: 0.032584473768857244\n",
      "Epoch: 51/200  Loss: 0.03132157099865355\n",
      "Epoch: 52/200  Loss: 0.043179730794680835\n",
      "Epoch: 53/200  Loss: 0.03965442399285406\n",
      "Epoch: 54/200  Loss: 0.04975150529437979\n",
      "Epoch: 55/200  Loss: 0.04355361606678997\n",
      "Epoch: 56/200  Loss: 0.03902046000634514\n",
      "Epoch: 57/200  Loss: 0.03373102502351734\n",
      "Epoch: 58/200  Loss: 0.03578180092204474\n",
      "Epoch: 59/200  Loss: 0.04386180070023484\n",
      "Epoch: 60/200  Loss: 0.03510293678236929\n",
      "Epoch: 61/200  Loss: 0.04433151805281914\n",
      "Epoch: 62/200  Loss: 0.03853242624227352\n",
      "Epoch: 63/200  Loss: 0.04235812580279964\n",
      "Epoch: 64/200  Loss: 0.04377188643916577\n",
      "Epoch: 65/200  Loss: 0.03502645793538582\n",
      "Epoch: 66/200  Loss: 0.04639353956702643\n",
      "Epoch: 67/200  Loss: 0.031656726025158725\n",
      "Epoch: 68/200  Loss: 0.034728952905061033\n",
      "Epoch: 69/200  Loss: 0.03126207128473239\n",
      "Epoch: 70/200  Loss: 0.03217619821319316\n",
      "Epoch: 71/200  Loss: 0.03799817182004035\n",
      "Epoch: 72/200  Loss: 0.03637187999121314\n",
      "Epoch: 73/200  Loss: 0.04611687499449736\n",
      "Epoch: 74/200  Loss: 0.04666204647070531\n",
      "Epoch: 75/200  Loss: 0.032626941893493064\n",
      "Epoch: 76/200  Loss: 0.03947040972959208\n",
      "Epoch: 77/200  Loss: 0.03236253696106691\n",
      "Epoch: 78/200  Loss: 0.03924503998034257\n",
      "Epoch: 79/200  Loss: 0.027117576756795103\n",
      "Epoch: 80/200  Loss: 0.03066083133199892\n",
      "Epoch: 81/200  Loss: 0.036613588733083076\n",
      "Epoch: 82/200  Loss: 0.032315673533021734\n",
      "Epoch: 83/200  Loss: 0.03154792079067255\n",
      "Epoch: 84/200  Loss: 0.18644759617246545\n",
      "Epoch: 85/200  Loss: 0.10465385586972023\n",
      "Epoch: 86/200  Loss: 0.03388664146015487\n",
      "Epoch: 87/200  Loss: 0.03202429822624726\n",
      "Epoch: 88/200  Loss: 0.03328871746024527\n",
      "Epoch: 89/200  Loss: 0.030063898580413147\n",
      "Epoch: 90/200  Loss: 0.029360513161534926\n",
      "Epoch: 91/200  Loss: 0.028793612624014055\n",
      "Epoch: 92/200  Loss: 0.028430293900687725\n",
      "Epoch: 93/200  Loss: 0.027988878321377378\n",
      "Epoch: 94/200  Loss: 0.027747818522349414\n",
      "Epoch: 95/200  Loss: 0.027449150401989118\n",
      "Epoch: 96/200  Loss: 0.02730594890452041\n",
      "Epoch: 97/200  Loss: 0.02699580049084893\n",
      "Epoch: 98/200  Loss: 0.026963214075876145\n",
      "Epoch: 99/200  Loss: 0.026982530789309477\n",
      "Epoch: 100/200  Loss: 0.026386240691974688\n",
      "Epoch: 101/200  Loss: 0.02615512053245052\n",
      "Epoch: 102/200  Loss: 0.025922878206182238\n",
      "Epoch: 103/200  Loss: 0.02710060239254915\n",
      "Epoch: 104/200  Loss: 0.025480417617294835\n",
      "Epoch: 105/200  Loss: 0.02515758732323408\n",
      "Epoch: 106/200  Loss: 0.025075216019248905\n",
      "Epoch: 107/200  Loss: 0.02399735301588084\n",
      "Epoch: 108/200  Loss: 0.02447492809982699\n",
      "Epoch: 109/200  Loss: 0.026797300625405574\n",
      "Epoch: 110/200  Loss: 0.022692473062237307\n",
      "Epoch: 111/200  Loss: 0.022530638269912504\n",
      "Epoch: 112/200  Loss: 0.022269939246522763\n",
      "Epoch: 113/200  Loss: 0.0238179362631469\n",
      "Epoch: 114/200  Loss: 0.02314236486083215\n",
      "Epoch: 115/200  Loss: 0.022520721245232175\n",
      "Epoch: 116/200  Loss: 0.022107854377133616\n",
      "Epoch: 117/200  Loss: 0.022389827634078895\n",
      "Epoch: 118/200  Loss: 0.022506958526265174\n",
      "Epoch: 119/200  Loss: 0.022309047489802877\n",
      "Epoch: 120/200  Loss: 0.02208088859133107\n",
      "Epoch: 121/200  Loss: 0.022220765761977724\n",
      "Epoch: 122/200  Loss: 0.02209118633167501\n",
      "Epoch: 123/200  Loss: 0.021955790946994146\n",
      "Epoch: 124/200  Loss: 0.021802419671306195\n",
      "Epoch: 125/200  Loss: 0.021798188494596565\n",
      "Epoch: 126/200  Loss: 0.021884710223530644\n",
      "Epoch: 127/200  Loss: 0.021554364556455947\n",
      "Epoch: 128/200  Loss: 0.021524784023593972\n",
      "Epoch: 129/200  Loss: 0.02144619789324633\n",
      "Epoch: 130/200  Loss: 0.02171854040786307\n",
      "Epoch: 131/200  Loss: 0.02140145234785807\n",
      "Epoch: 132/200  Loss: 0.02152016871930149\n",
      "Epoch: 133/200  Loss: 0.02165796055633879\n",
      "Epoch: 134/200  Loss: 0.021435275049210053\n",
      "Epoch: 135/200  Loss: 0.02127338416868158\n",
      "Epoch: 136/200  Loss: 0.021643770177474268\n",
      "Epoch: 137/200  Loss: 0.021380007270246307\n",
      "Epoch: 138/200  Loss: 0.021137670623413703\n",
      "Epoch: 139/200  Loss: 0.02151203501429261\n",
      "Epoch: 140/200  Loss: 0.021307213791038\n",
      "Epoch: 141/200  Loss: 0.021203723726174652\n",
      "Epoch: 142/200  Loss: 0.021350670619929128\n",
      "Epoch: 143/200  Loss: 0.022946545644278487\n",
      "Epoch: 144/200  Loss: 0.021309072379081005\n",
      "Epoch: 145/200  Loss: 0.02109432453412685\n",
      "Epoch: 146/200  Loss: 0.021232587383465947\n",
      "Epoch: 147/200  Loss: 0.021174844565861817\n",
      "Epoch: 148/200  Loss: 0.021268872562406317\n",
      "Epoch: 149/200  Loss: 0.020463950441152397\n",
      "Epoch: 150/200  Loss: 0.021121345900367784\n",
      "Epoch: 151/200  Loss: 0.021113068434264245\n",
      "Epoch: 152/200  Loss: 0.020899920196494993\n",
      "Epoch: 153/200  Loss: 0.020602941682910856\n",
      "Epoch: 154/200  Loss: 0.020795732352774574\n",
      "Epoch: 155/200  Loss: 0.020695370198803666\n",
      "Epoch: 156/200  Loss: 0.020939197299965906\n",
      "Epoch: 157/200  Loss: 0.02079146828620632\n",
      "Epoch: 158/200  Loss: 0.020692758913125775\n",
      "Epoch: 159/200  Loss: 0.020572274603124406\n",
      "Epoch: 160/200  Loss: 0.020714712971500748\n",
      "Epoch: 161/200  Loss: 0.02077571758234754\n",
      "Epoch: 162/200  Loss: 0.022389445244063937\n",
      "Epoch: 163/200  Loss: 0.02074089805662883\n",
      "Epoch: 164/200  Loss: 0.0205106752740069\n",
      "Epoch: 165/200  Loss: 0.021275231560863476\n",
      "Epoch: 166/200  Loss: 0.02047519374907635\n",
      "Epoch: 167/200  Loss: 0.02094941976132976\n",
      "Epoch: 168/200  Loss: 0.020371590287114917\n",
      "Epoch: 169/200  Loss: 0.020392557005289427\n",
      "Epoch: 170/200  Loss: 0.020593418948680338\n",
      "Epoch: 171/200  Loss: 0.02039919733368001\n",
      "Epoch: 172/200  Loss: 0.020464773099196722\n",
      "Epoch: 173/200  Loss: 0.020381444258460342\n",
      "Epoch: 174/200  Loss: 0.020145565720165525\n",
      "Epoch: 175/200  Loss: 0.020350629494657956\n",
      "Epoch: 176/200  Loss: 0.020343196406974375\n",
      "Epoch: 177/200  Loss: 0.02037538788794583\n",
      "Epoch: 178/200  Loss: 0.0201387500409693\n",
      "Epoch: 179/200  Loss: 0.020138563667261262\n",
      "Epoch: 180/200  Loss: 0.020562486281306684\n",
      "Epoch: 181/200  Loss: 0.02004032518240876\n",
      "Epoch: 182/200  Loss: 0.02008254737642585\n",
      "Epoch: 183/200  Loss: 0.02010864095995189\n",
      "Epoch: 184/200  Loss: 0.020618914010086357\n",
      "Epoch: 185/200  Loss: 0.020272301297489158\n",
      "Epoch: 186/200  Loss: 0.020252339894583703\n",
      "Epoch: 187/200  Loss: 0.020025965757846972\n",
      "Epoch: 188/200  Loss: 0.020125469009441576\n",
      "Epoch: 189/200  Loss: 0.0198613776188976\n",
      "Epoch: 190/200  Loss: 0.020292992148690417\n",
      "Epoch: 191/200  Loss: 0.02064189714359046\n",
      "Epoch: 192/200  Loss: 0.020038691694531742\n",
      "Epoch: 193/200  Loss: 0.019993947098104245\n",
      "Epoch: 194/200  Loss: 0.019780050922411696\n",
      "Epoch: 195/200  Loss: 0.01979939896004915\n",
      "Epoch: 196/200  Loss: 0.020229334014132294\n",
      "Epoch: 197/200  Loss: 0.01978365185160854\n",
      "Epoch: 198/200  Loss: 0.020020063640836583\n",
      "Epoch: 199/200  Loss: 0.019688863622743556\n",
      "Epoch: 200/200  Loss: 0.01967149229557502\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "net.fit(x_train, y_train, epochs=200, optimizer=sgd, loss_func=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.predict(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_name = '1_json'\n",
    "xml_name = '1_xml'\n",
    "\n",
    "net.create_json(json_name)\n",
    "net.create_xml(xml_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
