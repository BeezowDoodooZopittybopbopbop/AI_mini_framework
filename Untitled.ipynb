{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utils\n",
    "import numpy as np\n",
    "import json\n",
    "from xml.dom import minidom\n",
    "import os\n",
    "import abc\n",
    "\n",
    "\n",
    "# mean squared error\n",
    "def mse(y_true, y_pred, status=''):\n",
    "    return mse_derivative(y_true, y_pred) if status=='derivative' else np.mean(np.power(y_true-y_pred, 2))\n",
    "\n",
    "\n",
    "# sigmoid\n",
    "def sigmoid(input_data, status=''):\n",
    "    return sigmoid_derivative(input_data) if status =='derivative' else 1/(1 + np.ex(-input_data))\n",
    "\n",
    "\n",
    "# tg() activation function \n",
    "def tanh(input_data, status=''):\n",
    "    return tanh_derivative(input_data) if status=='derivative' else np.tanh(input_data);\n",
    "\n",
    "\n",
    "#  relu activation function\n",
    "def relu(input_data, status=''):\n",
    "    if status=='derivative':\n",
    "        return relu_derivative(input_data)\n",
    "    data = [max(0.05*value, value) for array in input_data for value in array]\n",
    "    return np.array(data).reshape(input_data.shape)\n",
    "\n",
    "\n",
    "# derivative of mean squared error\n",
    "def mse_derivative(y_true, y_pred):\n",
    "    return 2*(y_pred-y_true)/y_true.size\n",
    "\n",
    "\n",
    "# derivative of sigmoid\n",
    "def sigmoid_derivative(input_data):\n",
    "    return sigmoid(input_data)*(1 - sigmoid(input_data))\n",
    "\n",
    "\n",
    "# derivative of tag() activation functions\n",
    "def tanh_derivative(input_data):\n",
    "    return 1-np.tanh(input_data)**2\n",
    "\n",
    "\n",
    "# derivative of relu\n",
    "def relu_derivative(input_data):\n",
    "    data = [1 if value > 0 else 0.05 for array in input_data for value in array]\n",
    "    return np.array(data).reshape(input_data.shape)\n",
    "\n",
    "\n",
    "# calculate gradient\n",
    "def calculate_gradients(input_data, weights, bias, loss):\n",
    "    # db==loss\n",
    "    input_loss = np.dot(loss, weights.T)\n",
    "    dw = np.dot(input_data.T, loss)\n",
    "\n",
    "    return input_loss, dw, loss\n",
    "\n",
    "\n",
    "# get the true value\n",
    "def get_value(array):\n",
    "    arr = np.where(array==np.amax(array))\n",
    "    return int(arr[0]) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Layer(abc.ABC):\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._input = None\n",
    "        self._output = None\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def forward(self, input_data):\n",
    "        pass\n",
    "    \n",
    "    @abc.abstractmethod\n",
    "    def backward(self, error, learning_rate):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Linear(Layer):\n",
    "    \n",
    "    def __init__(self, input_size, output_size):\n",
    "        self._weights = np.random.rand(input_size, output_size) - 0.5\n",
    "        self._bias = np.random.rand(1, output_size) - 0.5\n",
    "        self._m_dw = np.zeros((input_size, output_size))\n",
    "        self._m_db = np.zeros((1, output_size))\n",
    "        self._v_dw = np.zeros((input_size, output_size))\n",
    "        self._v_db = np.zeros((1, output_size))\n",
    "        \n",
    "    def forward(self, input_data):\n",
    "        self._input = input_data\n",
    "        self._output = np.dot(self._input, self._weights) + self._bias\n",
    "        \n",
    "        return self._output\n",
    "\n",
    "    def backward(self, loss, optimizer):\n",
    "        self._weights, self._bias, input_loss, self._m_dw, self._m_db, self._v_dw, self._v_db = optimizer.update(self._input, self._weights, self._bias, loss, \n",
    "                                                                                                                 self._m_dw, self._m_db, self._v_dw, self._v_db)\n",
    "        \n",
    "        return input_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Activation(Layer):\n",
    "    \n",
    "    def __init__(self, activation_function):\n",
    "        self._activation_function = activation_function\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        self._input = input_data\n",
    "        self._output = self._activation_function(self._input)\n",
    "        \n",
    "        return self._output\n",
    "\n",
    "    def backward(self, loss, optimizer):\n",
    "        return self._activation_function(self._input, 'derivative') * loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self._layers = []\n",
    "        self._json = {}\n",
    "        self._root = minidom.Document()\n",
    "        \n",
    "        self._xml_results = self._root.createElement('results') \n",
    "        self._root.appendChild(self._xml_results)\n",
    "\n",
    "    def add(self, layer):\n",
    "        self._layers.append(layer)\n",
    "\n",
    "    def fit(self, x_train, y_train, epochs, optimizer, loss_func):\n",
    "        # dictionary for epoch results storing\n",
    "        d = {}\n",
    "        \n",
    "        for epoch in range(epochs):\n",
    "            loss_display = 0\n",
    "            for i in range(len(x_train)):\n",
    "                output = x_train[i]\n",
    "                \n",
    "                # forward propagation\n",
    "                for layer in self._layers:\n",
    "                    output = layer.forward(output)\n",
    "                \n",
    "                # calculating errors (1st to display, 2nd for backpropagataion)\n",
    "                loss_display += loss_func(y_train[i], output)\n",
    "                loss = loss_func(y_train[i], output, 'derivative')\n",
    "                \n",
    "                # backward propagation\n",
    "                for layer in reversed(self._layers):\n",
    "                    loss = layer.backward(loss, optimizer)\n",
    "            \n",
    "            loss_epoch = loss_display/len(x_train)\n",
    "            print('Epoch: {}/{}  Loss: {}'.format(epoch + 1, epochs, loss_epoch))\n",
    "            \n",
    "            d['Epoch: {}/{}'.format(epoch + 1, epochs)] = loss_epoch\n",
    "            self._xml_results.appendChild(self.xml_element('epoch: {}/{}'.format(epoch + 1, epochs), 'loss', str(loss_epoch)))\n",
    "        \n",
    "        self._json['Training'] = d\n",
    "        self._json['Optimizer'] = type(optimizer).__name__\n",
    "        self._json['Loss Function'] = loss_func.__name__\n",
    "        \n",
    "        self._xml_results.appendChild(self.xml_element('optimizer', 'name', str(type(optimizer).__name__)))\n",
    "        self._xml_results.appendChild(self.xml_element('loss function', 'name', str(loss_func.__name__)))\n",
    "            \n",
    "    def predict(self, input_data, true_value):\n",
    "        predictions = []\n",
    "        \n",
    "        # run network over all samples\n",
    "        for i in range(len(input_data)):\n",
    "            # forward propagation\n",
    "            output = input_data[i]\n",
    "            for layer in self._layers:\n",
    "                output = layer.forward(output)\n",
    "                \n",
    "            output = output.flatten()\n",
    "            predictions.append(get_value(output)==get_value(true_value[i]))\n",
    "            \n",
    "        accuracy = (sum(predictions)/len(predictions))*100\n",
    "        print('Accuracy: {}'.format(accuracy))\n",
    "        self._json['Accuracy'] = accuracy\n",
    "        self._xml_results.appendChild(self.xml_element('accuracy', 'value', str(accuracy)))\n",
    "        \n",
    "        return accuracy\n",
    "    \n",
    "    def create_json(self, fname):\n",
    "        json_object = json.dumps(self._json, indent = 4)\n",
    "        \n",
    "        with open(fname, \"w\") as outfile:\n",
    "            outfile.write(json_object)\n",
    "    \n",
    "    def create_xml(self, fname):\n",
    "        xml_str = self._root.toprettyxml(indent =\"\\t\") \n",
    "        \n",
    "        with open(fname, \"w\") as f:\n",
    "            f.write(xml_str) \n",
    "    \n",
    "    def xml_element(self, name_element, name_info, info):\n",
    "        xml_element = self._root.createElement(name_element)\n",
    "        xml_element.setAttribute(name_info, info)\n",
    "        return xml_element"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SGD():\n",
    "    \n",
    "    def __init__(self, learning_rate=0.1):\n",
    "        self._learning_rate = learning_rate\n",
    "        \n",
    "    def update(self, input_data, weights, bias, loss, m_dw, m_db, v_dw, v_db):\n",
    "        input_loss, dw, db = calculate_gradients(input_data, weights, bias, loss)\n",
    "\n",
    "        weights -= self._learning_rate * dw\n",
    "        bias -= self._learning_rate * db\n",
    "        \n",
    "        return weights, bias, input_loss, m_dw, m_db, v_dw, v_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Adam():\n",
    "    \n",
    "    def __init__(self, learning_rate=0.01, beta1=0.9, beta2=0.999, epsilon=1e-8):\n",
    "        self._learning_rate = learning_rate\n",
    "        self._beta1 = beta1\n",
    "        self._beta2 = beta2\n",
    "        self._epsilon = epsilon\n",
    "        \n",
    "    def update(self, input_data, weights, bias, loss, m_dw, m_db, v_dw, v_db):\n",
    "        input_loss, dw, db = calculate_gradients(input_data, weights, bias, loss)\n",
    "        \n",
    "        m_dw = self._beta1*m_dw + (1 - self._beta1)*dw\n",
    "        m_db = self._beta1*m_db + (1 - self._beta1)*db\n",
    "\n",
    "        v_dw = self._beta2*v_dw + (1 - self._beta2)*(dw**2)\n",
    "        v_db = self._beta2*v_db + (1 - self._beta2)*(db**2)\n",
    "\n",
    "        m_dw_corr = m_dw/(1-self._beta1)\n",
    "        m_db_corr = m_db/(1-self._beta1)\n",
    "        v_dw_corr = v_dw/(1-self._beta2)\n",
    "        v_db_corr = v_db/(1-self._beta2)\n",
    "\n",
    "        weights -= self._learning_rate*(m_dw_corr/(np.sqrt(v_dw_corr) + self._epsilon))\n",
    "        bias -= self._learning_rate*(m_db_corr/(np.sqrt(v_db_corr) + self._epsilon))\n",
    "        \n",
    "        return weights, bias, input_loss, m_dw, m_db, v_dw, v_db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import load_iris\n",
    "from keras.utils import np_utils\n",
    "\n",
    "\n",
    "# load data \n",
    "iris = load_iris()\n",
    "x, y = np.array(iris.data), np.array(iris.target)\n",
    "\n",
    "# split on training and test data\n",
    "x_train, x_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=1)\n",
    "\n",
    "# preprocess train and test data\n",
    "x_train = x_train.reshape(x_train.shape[0], 1, 4)\n",
    "x_train = x_train.astype('float32')\n",
    "y_train = np_utils.to_categorical(y_train)\n",
    "\n",
    "x_test = x_test.reshape(x_test.shape[0], 1, 4)\n",
    "x_test = x_test.astype('float32')\n",
    "y_test = np_utils.to_categorical(y_test)\n",
    "\n",
    "# optimizer\n",
    "sgd = SGD(learning_rate=0.01)\n",
    "adam = Adam(learning_rate=0.01)\n",
    "\n",
    "# network\n",
    "net = NeuralNetwork()\n",
    "net.add(Linear(4, 20))\n",
    "net.add(Activation(relu))\n",
    "net.add(Linear(20, 10))\n",
    "net.add(Activation(relu))\n",
    "net.add(Linear(10, 3))\n",
    "net.add(Activation(relu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1/100  Loss: 0.18186304938552186\n",
      "Epoch: 2/100  Loss: 0.05251118388487307\n",
      "Epoch: 3/100  Loss: 0.043076206663342935\n",
      "Epoch: 4/100  Loss: 0.02876804423937211\n",
      "Epoch: 5/100  Loss: 0.025032998570846166\n",
      "Epoch: 6/100  Loss: 0.02371580694490537\n",
      "Epoch: 7/100  Loss: 0.02296920560364677\n",
      "Epoch: 8/100  Loss: 0.022237537983738197\n",
      "Epoch: 9/100  Loss: 0.021525611737590888\n",
      "Epoch: 10/100  Loss: 0.020922263255289602\n",
      "Epoch: 11/100  Loss: 0.02025119987935173\n",
      "Epoch: 12/100  Loss: 0.019874016040320668\n",
      "Epoch: 13/100  Loss: 0.019197166589848966\n",
      "Epoch: 14/100  Loss: 0.019375967236736312\n",
      "Epoch: 15/100  Loss: 0.01923813551194527\n",
      "Epoch: 16/100  Loss: 0.01922730466280797\n",
      "Epoch: 17/100  Loss: 0.019026371930464858\n",
      "Epoch: 18/100  Loss: 0.01887673327322249\n",
      "Epoch: 19/100  Loss: 0.01870447854025502\n",
      "Epoch: 20/100  Loss: 0.0185956130930242\n",
      "Epoch: 21/100  Loss: 0.018389311442740402\n",
      "Epoch: 22/100  Loss: 0.018246299526400833\n",
      "Epoch: 23/100  Loss: 0.018076596412157243\n",
      "Epoch: 24/100  Loss: 0.017951360158947223\n",
      "Epoch: 25/100  Loss: 0.01777049420772233\n",
      "Epoch: 26/100  Loss: 0.017753956239388257\n",
      "Epoch: 27/100  Loss: 0.017559322607600573\n",
      "Epoch: 28/100  Loss: 0.017441318868947578\n",
      "Epoch: 29/100  Loss: 0.017302770688174082\n",
      "Epoch: 30/100  Loss: 0.018092521290350702\n",
      "Epoch: 31/100  Loss: 0.01767752347475759\n",
      "Epoch: 32/100  Loss: 0.017778156333892996\n",
      "Epoch: 33/100  Loss: 0.01722876796490141\n",
      "Epoch: 34/100  Loss: 0.016946030348677883\n",
      "Epoch: 35/100  Loss: 0.017056097922910875\n",
      "Epoch: 36/100  Loss: 0.01726508842505643\n",
      "Epoch: 37/100  Loss: 0.016842654621521385\n",
      "Epoch: 38/100  Loss: 0.016788532274814826\n",
      "Epoch: 39/100  Loss: 0.016850664222283523\n",
      "Epoch: 40/100  Loss: 0.01657100098031151\n",
      "Epoch: 41/100  Loss: 0.016384450317323905\n",
      "Epoch: 42/100  Loss: 0.01664073554661465\n",
      "Epoch: 43/100  Loss: 0.016881164482642784\n",
      "Epoch: 44/100  Loss: 0.016775076274684487\n",
      "Epoch: 45/100  Loss: 0.016621985659008704\n",
      "Epoch: 46/100  Loss: 0.016792377328624073\n",
      "Epoch: 47/100  Loss: 0.016462863653360634\n",
      "Epoch: 48/100  Loss: 0.016020014907916786\n",
      "Epoch: 49/100  Loss: 0.01609684512312239\n",
      "Epoch: 50/100  Loss: 0.01599912880710131\n",
      "Epoch: 51/100  Loss: 0.01588071471711042\n",
      "Epoch: 52/100  Loss: 0.01580594249183751\n",
      "Epoch: 53/100  Loss: 0.01557591177238656\n",
      "Epoch: 54/100  Loss: 0.015738636428159168\n",
      "Epoch: 55/100  Loss: 0.015787625681080532\n",
      "Epoch: 56/100  Loss: 0.015334113964664253\n",
      "Epoch: 57/100  Loss: 0.015399103122743086\n",
      "Epoch: 58/100  Loss: 0.01519504193743705\n",
      "Epoch: 59/100  Loss: 0.015224725444533783\n",
      "Epoch: 60/100  Loss: 0.015131611132432759\n",
      "Epoch: 61/100  Loss: 0.015059917074163032\n",
      "Epoch: 62/100  Loss: 0.015280632886915686\n",
      "Epoch: 63/100  Loss: 0.016355669314806996\n",
      "Epoch: 64/100  Loss: 0.014946623132941393\n",
      "Epoch: 65/100  Loss: 0.015249764239108324\n",
      "Epoch: 66/100  Loss: 0.015159960753407241\n",
      "Epoch: 67/100  Loss: 0.015221542349441857\n",
      "Epoch: 68/100  Loss: 0.01522371545795886\n",
      "Epoch: 69/100  Loss: 0.015108177010341164\n",
      "Epoch: 70/100  Loss: 0.015267501371794674\n",
      "Epoch: 71/100  Loss: 0.016266709377558197\n",
      "Epoch: 72/100  Loss: 0.014925371540537294\n",
      "Epoch: 73/100  Loss: 0.015496014040342102\n",
      "Epoch: 74/100  Loss: 0.015091820668767667\n",
      "Epoch: 75/100  Loss: 0.015158420345846553\n",
      "Epoch: 76/100  Loss: 0.01506314852582056\n",
      "Epoch: 77/100  Loss: 0.015165221915032228\n",
      "Epoch: 78/100  Loss: 0.014958183651983389\n",
      "Epoch: 79/100  Loss: 0.015248992446167476\n",
      "Epoch: 80/100  Loss: 0.014989590822715523\n",
      "Epoch: 81/100  Loss: 0.01511752100898956\n",
      "Epoch: 82/100  Loss: 0.015172464310336338\n",
      "Epoch: 83/100  Loss: 0.015041725647142564\n",
      "Epoch: 84/100  Loss: 0.015245486911938867\n",
      "Epoch: 85/100  Loss: 0.01521901200216937\n",
      "Epoch: 86/100  Loss: 0.01480278438735151\n",
      "Epoch: 87/100  Loss: 0.015172364201037434\n",
      "Epoch: 88/100  Loss: 0.015106385574093622\n",
      "Epoch: 89/100  Loss: 0.01547150843643399\n",
      "Epoch: 90/100  Loss: 0.015190088273013146\n",
      "Epoch: 91/100  Loss: 0.014679230745795763\n",
      "Epoch: 92/100  Loss: 0.015118086453830417\n",
      "Epoch: 93/100  Loss: 0.01475319545670232\n",
      "Epoch: 94/100  Loss: 0.01500399433786684\n",
      "Epoch: 95/100  Loss: 0.015283834989334787\n",
      "Epoch: 96/100  Loss: 0.015597479079878495\n",
      "Epoch: 97/100  Loss: 0.015407857130280676\n",
      "Epoch: 98/100  Loss: 0.015957708098158566\n",
      "Epoch: 99/100  Loss: 0.015933670665848453\n",
      "Epoch: 100/100  Loss: 0.015025325704450154\n"
     ]
    }
   ],
   "source": [
    "# train\n",
    "net.fit(x_train, y_train, epochs=100, optimizer=adam, loss_func=mse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 100.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "100.0"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.predict(x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "json_name = '1_json'\n",
    "xml_name = '1_xml'\n",
    "\n",
    "net.create_json(json_name)\n",
    "net.create_xml(xml_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
